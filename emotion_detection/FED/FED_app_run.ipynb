{"cells":[{"cell_type":"code","source":[],"metadata":{"id":"XLrrwpL_K06R"},"id":"XLrrwpL_K06R","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"50296535","metadata":{"id":"50296535"},"source":["# Importing Libraries"]},{"cell_type":"code","execution_count":1,"id":"fa256e39","metadata":{"id":"fa256e39","executionInfo":{"status":"ok","timestamp":1664104921043,"user_tz":-420,"elapsed":2201,"user":{"displayName":"Xuan Hoang Nguyen","userId":"17477451880673205891"}}},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from tensorflow.keras.models import load_model, model_from_json\n","from tensorflow.keras.preprocessing.image import load_img,img_to_array\n","from tensorflow.keras.preprocessing import image"]},{"cell_type":"markdown","id":"0d909aee","metadata":{"id":"0d909aee"},"source":["# Setting up the model & cv2 with haar_cascade classifier"]},{"cell_type":"code","execution_count":null,"id":"3ab90ea9","metadata":{"id":"3ab90ea9"},"outputs":[],"source":["model = model_from_json(open(\"./model.json\", \"r\").read()) #reading the saved model from training\n","model.load_weights('./model.h5') #loading the weights from saved file after training\n","classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') #initializing classifier to detect faces using haar_cascade\n","cap = cv2.VideoCapture(0) #initializing camera using cv2 to capture face"]},{"cell_type":"code","execution_count":null,"id":"1085a996","metadata":{"id":"1085a996"},"outputs":[],"source":["while cap.isOpened():\n","    #setting up the camera frame and UI\n","    res, frame = cap.read()\n","    height, width , channel = frame.shape\n","    sub_img = frame[0:int(height / 9),0:int(width)]\n","    rect = np.ones(sub_img.shape, dtype = np.uint8) * 0\n","    res = cv2.addWeighted(sub_img, 0.77, rect, 0.23, 0)\n","    \n","    #styling font and label properties\n","    FONT = cv2.FONT_HERSHEY_TRIPLEX\n","    FONT_SCALE = 0\n","    FONT_THICKNESS = 0\n","    label_color = (64, 255, 0)\n","    label = \"\"\n","    \n","    #positioning\n","    label_dimension = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICKNESS)[0]\n","    textX = int((res.shape[1] - label_dimension[0]) / 2)\n","    textY = int((res.shape[0] + label_dimension[1]) / 2)\n","    cv2.putText(res, label, (textX, textY), FONT, FONT_SCALE, (255, 255, 255), FONT_THICKNESS)\n","    \n","    #capturing faces and converting the images to grayscale\n","    gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    faces = classifier.detectMultiScale(gray_image)\n","    \n","    try:\n","        for (x, y, w, h) in faces:\n","            #selecting Region of Interest(ROI) from camera\n","            cv2.rectangle(frame, pt1 = (x, y), pt2 = (x+w, y+h), color = (64, 255, 0),thickness =  2)\n","            roi_gray = gray_image[y-5 : y+h+5, x-5 : x+w+5]\n","            roi_gray = cv2.resize(roi_gray, (48,48))\n","            image_pixels = img_to_array(roi_gray) #converting image to array for prediction of emotions\n","            image_pixels = np.expand_dims(image_pixels, axis = 0)\n","            image_pixels /= 255\n","            predictions = model.predict(image_pixels) #using the model to predict emotions\n","            max_index = np.argmax(predictions[0]) #storing max prediction value to max_index\n","            emotions = ('Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral')\n","            emotion_prediction = emotions[max_index] #selecting matching prediction from emotions tuple\n","            \n","            #displaying prediction and accuracy as labels in UI\n","            cv2.putText(res, \"Emotion: {}\".format(emotion_prediction), (40, textY+5+5), FONT, 0.7, label_color, 1)\n","            label_violation = 'Accuracy: {}'.format(str(np.round(np.max(predictions[0]) * 100, 1)) + \"%\")\n","            cv2.putText(res, label_violation, (400, textY+5+5), FONT, 0.7, label_color, 1)\n","    except:\n","        pass\n","    \n","    frame[0:int(height / 9), 0:int(width)] = res\n","    cv2.imshow('Face Emotion Detector (19033540)', frame) #frame (window) title\n","\n","    #exit application on pressing 'q'\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","        \n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"id":"38cb580a","metadata":{"id":"38cb580a"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}